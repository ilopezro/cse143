{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilopezro/cse143/blob/jen-assg2/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVh-wYClTi5Y",
        "colab_type": "text"
      },
      "source": [
        "## **CSE 143: Assignment 2** <br>\n",
        "Professor Jeffrey Flannagan <br>\n",
        "1/30/2020 \n",
        "<br><br>\n",
        "Isai Lopez Rodas <br>\n",
        "ilopezro \n",
        "<br><br>\n",
        "Jennifer Dutra <br>\n",
        "jrdutra\n",
        "<br><br>\n",
        "Khang Tran <br>\n",
        "khvitran"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWq7VwDP5aml",
        "colab_type": "text"
      },
      "source": [
        "# Setup <br>\n",
        "Taken directly from Professor's Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BRqvtIE5OvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    !pip install -q -U tensorflow-addons\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"nlp\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6INDQZSvbUVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca4ScLy4lDcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "#splitting of training and dev data\n",
        "train_data, dev_data, test_data = tfds.load(\"imdb_reviews\", split=('train[:60%]', 'train[60%:]', 'test'), as_supervised=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WUbTlp5T9ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size = len(list(train_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkRdxz20mFTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(X_batch, y_batch):\n",
        "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
        "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
        "    X_batch = tf.strings.split(X_batch)\n",
        "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_mCbbWCmZQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# preprocessing training data \n",
        "vocabulary = Counter()\n",
        "for X_batch, y_batch in train_data.batch(32).map(preprocess):\n",
        "    for review in X_batch:\n",
        "        vocabulary.update(list(review.numpy()))\n",
        "        \n",
        "#preprocessing dev data\n",
        "devVocabulary = Counter()\n",
        "for X_batch, y_batch in dev_data.batch(32).map(preprocess):\n",
        "    for review in X_batch:\n",
        "        devVocabulary.update(list(review.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEd-C40Fmd5w",
        "colab_type": "code",
        "outputId": "cebafe2f-e115-4fee-a9cb-0ba2da898545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(vocabulary.most_common()[:3])\n",
        "print(devVocabulary.most_common()[:3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(b'<pad>', 128536), (b'the', 36691), (b'a', 22997)]\n",
            "[(b'<pad>', 85653), (b'the', 24446), (b'a', 15567)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPGlIfjmnsbQ",
        "colab_type": "code",
        "outputId": "a07416e6-78ed-4d9d-cdf8-58b5f9102b0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(vocabulary))\n",
        "print(len(devVocabulary))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41624\n",
            "34138\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4u393r6owsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 10000\n",
        "truncated_vocabulary = [\n",
        "    word for word, count in vocabulary.most_common()[:vocab_size]]\n",
        "truncated_dev_vocabulary = [\n",
        "    word for word, count in devVocabulary.most_common()[:vocab_size]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D7Rsv23vMpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_id_train = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
        "word_to_id_dev = {word: index for index, word in enumerate(truncated_dev_vocabulary)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuEbH2FnvX1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = tf.constant(truncated_vocabulary)\n",
        "dev_words = tf.constant(truncated_dev_vocabulary)\n",
        "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
        "word_ids_dev = tf.range(len(truncated_dev_vocabulary), dtype=tf.int64)\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "vocab_init_dev = tf.lookup.KeyValueTensorInitializer(dev_words, word_ids_dev)\n",
        "num_oov_buckets = 1000\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
        "dev_table = tf.lookup.StaticVocabularyTable(vocab_init_dev, num_oov_buckets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpiahr_YvaNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_words(X_batch, y_batch):\n",
        "    return table.lookup(X_batch), y_batch\n",
        "  \n",
        "def encode_words_dev(X_batch, y_batch):\n",
        "    return dev_table.lookup(X_batch), y_batch\n",
        "\n",
        "train_set = train_data.repeat().batch(512).map(preprocess)\n",
        "train_set = train_set.map(encode_words).prefetch(1)\n",
        "dev_set = dev_data.batch(512).map(preprocess)\n",
        "dev_set = dev_set.map(encode_words_dev).prefetch(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1gAQaM8vugH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for X_batch, y_batch in train_set.take(1):\n",
        "    print(X_batch)\n",
        "    print(y_batch)\n",
        "for X_batch, y_batch in dev_set.take(1):\n",
        "    print(X_batch)\n",
        "    print(y_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnGZTt4MwGMC",
        "colab_type": "code",
        "outputId": "f9af3a8b-c4df-4b6e-ac5d-92d0b761f401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "embed_size = 128\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
        "                           mask_zero=True, # not shown in the book\n",
        "                           input_shape=[None]),\n",
        "    keras.layers.SimpleRNN(128),\n",
        "    # keras.layers.Dropout(0.1),\n",
        "    # we can also call dropout inside simpleRNN and change activation of \n",
        "    # simple rnn inside. the default = tanh\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(), metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, steps_per_epoch=train_size // 32, epochs=5, validation_data=dev_set)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 468 steps\n",
            "Epoch 1/5\n",
            "468/468 [==============================] - 70s 149ms/step - loss: 0.0999 - accuracy: 0.9568 - val_loss: 2.1490 - val_accuracy: 0.5167\n",
            "Epoch 2/5\n",
            "468/468 [==============================] - 67s 143ms/step - loss: 2.0137e-04 - accuracy: 1.0000 - val_loss: 2.4233 - val_accuracy: 0.5151\n",
            "Epoch 3/5\n",
            "468/468 [==============================] - 71s 152ms/step - loss: 7.7913e-05 - accuracy: 1.0000 - val_loss: 2.5883 - val_accuracy: 0.5144\n",
            "Epoch 4/5\n",
            "468/468 [==============================] - 72s 155ms/step - loss: 4.1618e-05 - accuracy: 1.0000 - val_loss: 2.7132 - val_accuracy: 0.5143\n",
            "Epoch 5/5\n",
            "468/468 [==============================] - 73s 157ms/step - loss: 2.5442e-05 - accuracy: 1.0000 - val_loss: 2.8157 - val_accuracy: 0.5143\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}